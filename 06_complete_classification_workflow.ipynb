{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON0W61b6gdPP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles, make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Part 1: Complete Binary Classification Workflow\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Create data\n",
        "print(\"\\nStep 1: Creating data...\")\n",
        "X, y = make_circles(n_samples=1000, noise=0.03, factor=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Split data\n",
        "print(\"Step 2: Splitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Convert to tensors\n",
        "print(\"Step 3: Converting to tensors...\")\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_test = torch.FloatTensor(y_test).unsqueeze(1)\n",
        "\n",
        "# Step 4: Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Step 4: Using device: {device}\")\n",
        "\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Step 5: Define model\n",
        "print(\"Step 5: Building model...\")\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, hidden_units=[16, 8]):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = 2\n",
        "        for hidden_size in hidden_units:\n",
        "            layers.extend([nn.Linear(prev_size, hidden_size), nn.ReLU()])\n",
        "            prev_size = hidden_size\n",
        "        layers.append(nn.Linear(prev_size, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "model = BinaryClassifier(hidden_units=[16, 8]).to(device)\n",
        "\n",
        "# Step 6: Define loss and optimizer\n",
        "print(\"Step 6: Setting up loss and optimizer...\")\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Step 7: Train model\n",
        "print(\"Step 7: Training model...\")\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    y_logits = model(X_train)\n",
        "    loss = criterion(y_logits, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        test_logits = model(X_test)\n",
        "        test_loss = criterion(test_logits, y_test)\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f'  Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
        "\n",
        "# Step 8: Evaluate\n",
        "print(\"Step 8: Evaluating model...\")\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    test_probs = torch.sigmoid(model(X_test))\n",
        "    test_preds = (test_probs > 0.5).long()\n",
        "\n",
        "accuracy = (test_preds == y_test).float().mean()\n",
        "print(f\"Binary Classification Accuracy: {accuracy.item()*100:.2f}%\")"
      ],
      "metadata": {
        "id": "BwluSjZ3ivRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 2: Complete Multi-Class Workflow\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create multi-class data\n",
        "print(\"\\nCreating multi-class data...\")\n",
        "X_multi, y_multi = make_blobs(n_samples=1000, n_features=2, centers=4, random_state=42)\n",
        "\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
        "    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
        ")\n",
        "\n",
        "X_train_m = torch.FloatTensor(X_train_m)\n",
        "y_train_m = torch.LongTensor(y_train_m)\n",
        "X_test_m = torch.FloatTensor(X_test_m)\n",
        "y_test_m = torch.LongTensor(y_test_m)\n",
        "\n",
        "X_train_m, y_train_m = X_train_m.to(device), y_train_m.to(device)\n",
        "X_test_m, y_test_m = X_test_m.to(device), y_test_m.to(device)\n",
        "\n",
        "# Define multi-class model\n",
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=4, hidden_units=[16, 8]):\n",
        "        super(MultiClassClassifier, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = 2\n",
        "        for hidden_size in hidden_units:\n",
        "            layers.extend([nn.Linear(prev_size, hidden_size), nn.ReLU()])\n",
        "            prev_size = hidden_size\n",
        "        layers.append(nn.Linear(prev_size, num_classes))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "model_m = MultiClassClassifier(num_classes=4, hidden_units=[16, 8]).to(device)\n",
        "\n",
        "# Train\n",
        "print(\"Training multi-class model...\")\n",
        "criterion_m = nn.CrossEntropyLoss()\n",
        "optimizer_m = optim.Adam(model_m.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    model_m.train()\n",
        "    outputs = model_m(X_train_m)\n",
        "    loss = criterion_m(outputs, y_train_m)\n",
        "    optimizer_m.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_m.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f'  Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate\n",
        "model_m.eval()\n",
        "with torch.inference_mode():\n",
        "    test_preds_m = torch.argmax(model_m(X_test_m), dim=1)\n",
        "\n",
        "accuracy_m = (test_preds_m == y_test_m).float().mean()\n",
        "print(f\"Multi-class Accuracy: {accuracy_m.item()*100:.2f}%\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oK8ZDjdgi1US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 3: Hyperparameter Experimentation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def train_and_evaluate(hidden_units, learning_rate, epochs=100):\n",
        "    \"\"\"Train model with given hyperparameters\"\"\"\n",
        "    # Create model\n",
        "    model = BinaryClassifier(hidden_units=hidden_units).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        y_logits = model(X_train)\n",
        "        loss = criterion(y_logits, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        test_preds = (torch.sigmoid(model(X_test)) > 0.5).long()\n",
        "        accuracy = (test_preds == y_test).float().mean()\n",
        "\n",
        "    return accuracy.item()\n",
        "\n",
        "# Experiment with different configurations\n",
        "configs = [\n",
        "    {'hidden_units': [8], 'lr': 0.01},\n",
        "    {'hidden_units': [16], 'lr': 0.01},\n",
        "    {'hidden_units': [16, 8], 'lr': 0.01},\n",
        "    {'hidden_units': [32, 16], 'lr': 0.001},\n",
        "    {'hidden_units': [64, 32, 16], 'lr': 0.001},\n",
        "]\n",
        "\n",
        "print(\"\\nTesting different configurations:\")\n",
        "results = []\n",
        "for i, config in enumerate(configs):\n",
        "    acc = train_and_evaluate(config['hidden_units'], config['lr'])\n",
        "    results.append({**config, 'accuracy': acc})\n",
        "    print(f\"  Config {i+1}: hidden={config['hidden_units']}, lr={config['lr']}, Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "# Find best configuration\n",
        "best_config = max(results, key=lambda x: x['accuracy'])\n",
        "print(f\"\\nBest configuration: {best_config}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oyRsMyQui5Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 4: Model Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train multiple models\n",
        "models = {\n",
        "    'Linear': BinaryClassifier(hidden_units=[]),\n",
        "    'Small': BinaryClassifier(hidden_units=[8]),\n",
        "    'Medium': BinaryClassifier(hidden_units=[16, 8]),\n",
        "    'Large': BinaryClassifier(hidden_units=[32, 16]),\n",
        "}\n",
        "\n",
        "print(\"\\nComparing different model sizes:\")\n",
        "comparison_results = {}\n",
        "\n",
        "for name, model_comp in models.items():\n",
        "    model_comp = model_comp.to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model_comp.parameters(), lr=0.01)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(100):\n",
        "        model_comp.train()\n",
        "        y_logits = model_comp(X_train)\n",
        "        loss = criterion(y_logits, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate\n",
        "    model_comp.eval()\n",
        "    with torch.inference_mode():\n",
        "        test_preds = (torch.sigmoid(model_comp(X_test)) > 0.5).long()\n",
        "        accuracy = (test_preds == y_test).float().mean()\n",
        "        params = sum(p.numel() for p in model_comp.parameters())\n",
        "\n",
        "    comparison_results[name] = {\n",
        "        'accuracy': accuracy.item(),\n",
        "        'parameters': params\n",
        "    }\n",
        "    print(f\"  {name:8s}: Accuracy={accuracy.item()*100:5.2f}%, Parameters={params:,}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iPWCCO2fjGFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 5: Saving and Loading Models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create directory for saving models\n",
        "model_dir = Path('saved_models')\n",
        "model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save model state dict\n",
        "model_path = model_dir / 'binary_classifier.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "# Load model\n",
        "loaded_model = BinaryClassifier(hidden_units=[16, 8])\n",
        "loaded_model.load_state_dict(torch.load(model_path))\n",
        "loaded_model = loaded_model.to(device)\n",
        "loaded_model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Verify loaded model works\n",
        "with torch.inference_mode():\n",
        "    test_preds_loaded = (torch.sigmoid(loaded_model(X_test)) > 0.5).long()\n",
        "    accuracy_loaded = (test_preds_loaded == y_test).float().mean()\n",
        "print(f\"Loaded model accuracy: {accuracy_loaded.item()*100:.2f}%\")\n",
        "\n",
        "# Save complete model (including architecture)\n",
        "complete_model_path = model_dir / 'binary_classifier_complete.pth'\n",
        "torch.save(model, complete_model_path)\n",
        "print(f\"Complete model saved to: {complete_model_path}\")\n",
        "\n",
        "# Load complete model\n",
        "loaded_complete = torch.load(complete_model_path, weights_only=False)\n",
        "loaded_complete.eval()\n",
        "print(\"Complete model loaded successfully!\")"
      ],
      "metadata": {
        "id": "SSBY5htzjJZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 6: Model Inference Function\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def predict(model, X_new, device='cpu'):\n",
        "    \"\"\"Make predictions on new data\"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Convert to tensor if needed\n",
        "    if not isinstance(X_new, torch.Tensor):\n",
        "        X_new = torch.FloatTensor(X_new)\n",
        "\n",
        "    X_new = X_new.to(device)\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.inference_mode():\n",
        "        logits = model(X_new)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > 0.5).long()\n",
        "\n",
        "    return {\n",
        "        'logits': logits.cpu(),\n",
        "        'probabilities': probs.cpu(),\n",
        "        'predictions': preds.cpu()\n",
        "    }\n",
        "\n",
        "# Test inference function\n",
        "X_new = torch.randn(5, 2)\n",
        "results = predict(model, X_new, device)\n",
        "\n",
        "print(\"\\nPredictions for 5 new samples:\")\n",
        "for i in range(5):\n",
        "    print(f\"  Sample {i+1}:\")\n",
        "    print(f\"    Input: {X_new[i].numpy()}\")\n",
        "    print(f\"    Probability: {results['probabilities'][i].item():.4f}\")\n",
        "    print(f\"    Predicted class: {results['predictions'][i].item()}\")\n"
      ],
      "metadata": {
        "id": "8p0xHF-sjM0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 7: Complete Training Class\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class ClassificationTrainer:\n",
        "    \"\"\"Complete training pipeline for classification\"\"\"\n",
        "\n",
        "    def __init__(self, model, device='cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.history = {'train_loss': [], 'test_loss': []}\n",
        "\n",
        "    def train(self, X_train, y_train, X_test, y_test,\n",
        "              epochs=100, lr=0.01):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            y_logits = self.model(X_train)\n",
        "            loss = criterion(y_logits, y_train)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Testing\n",
        "            self.model.eval()\n",
        "            with torch.inference_mode():\n",
        "                test_logits = self.model(X_test)\n",
        "                test_loss = criterion(test_logits, y_test)\n",
        "\n",
        "            self.history['train_loss'].append(loss.item())\n",
        "            self.history['test_loss'].append(test_loss.item())\n",
        "\n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluate the model\"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.inference_mode():\n",
        "            test_probs = torch.sigmoid(self.model(X_test))\n",
        "            test_preds = (test_probs > 0.5).long()\n",
        "            accuracy = (test_preds == y_test).float().mean()\n",
        "\n",
        "        return accuracy.item()\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Save the model\"\"\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"Model saved to: {path}\")\n",
        "\n",
        "    def load(self, path):\n",
        "        \"\"\"Load the model\"\"\"\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        print(f\"Model loaded from: {path}\")\n",
        "\n",
        "# Use the trainer\n",
        "print(\"\\nUsing ClassificationTrainer:\")\n",
        "model_trainer = BinaryClassifier(hidden_units=[16, 8])\n",
        "trainer = ClassificationTrainer(model_trainer, device=device)\n",
        "trainer.train(X_train, y_train, X_test, y_test, epochs=100, lr=0.01)\n",
        "accuracy_trainer = trainer.evaluate(X_test, y_test)\n",
        "print(f\"Final accuracy: {accuracy_trainer*100:.2f}%\")\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save(model_dir / 'trained_classifier.pth')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2Rdse28HjV3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 8: Training Curves Visualization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "axes[0].plot(train_losses, label='Train Loss')\n",
        "axes[0].plot(test_losses, label='Test Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss Curves')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Model comparison\n",
        "names = list(comparison_results.keys())\n",
        "accuracies = [comparison_results[name]['accuracy'] * 100 for name in names]\n",
        "axes[1].bar(names, accuracies)\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Model Comparison')\n",
        "axes[1].set_ylim(0, 100)\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[1].text(i, v + 2, f'{v:.1f}%', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BJl8kgg5jZA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Exercise 1: Design your own experiment\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define configurations to test\n",
        "experiment_configs = [\n",
        "    {\n",
        "        'name': 'Config A - Simple Architecture, High LR, Few Epochs',\n",
        "        'hidden_units': [8],\n",
        "        'learning_rate': 0.05,\n",
        "        'epochs': 50\n",
        "    },\n",
        "    {\n",
        "        'name': 'Config B - Medium Architecture, Medium LR, Medium Epochs',\n",
        "        'hidden_units': [16, 8],\n",
        "        'learning_rate': 0.01,\n",
        "        'epochs': 100\n",
        "    },\n",
        "    {\n",
        "        'name': 'Config C - Complex Architecture, Low LR, Many Epochs',\n",
        "        'hidden_units': [32, 16, 8],\n",
        "        'learning_rate': 0.001,\n",
        "        'epochs': 150\n",
        "    },\n",
        "    {\n",
        "        'name': 'Config D - Different Architecture, Medium LR, Medium Epochs',\n",
        "        'hidden_units': [64, 32],\n",
        "        'learning_rate': 0.01,\n",
        "        'epochs': 100\n",
        "    },\n",
        "]\n",
        "\n",
        "# Test each configuration\n",
        "experiment_results = []\n",
        "\n",
        "for config in experiment_configs:\n",
        "    print(f\"\\n{config['name']}\")\n",
        "    print(f\"  Architecture: {config['hidden_units']}\")\n",
        "    print(f\"  Learning Rate: {config['learning_rate']}\")\n",
        "    print(f\"  Epochs: {config['epochs']}\")\n",
        "\n",
        "    # Create a new model for this configuration\n",
        "    experiment_model = BinaryClassifier(hidden_units=config['hidden_units'])\n",
        "\n",
        "    # Create trainer\n",
        "    experiment_trainer = ClassificationTrainer(experiment_model, device=device)\n",
        "\n",
        "    # Train the model\n",
        "    experiment_trainer.train(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        epochs=config['epochs'],\n",
        "        lr=config['learning_rate']\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    experiment_accuracy = experiment_trainer.evaluate(X_test, y_test)\n",
        "\n",
        "    # Store results\n",
        "    result = {\n",
        "        'name': config['name'],\n",
        "        'hidden_units': config['hidden_units'],\n",
        "        'learning_rate': config['learning_rate'],\n",
        "        'epochs': config['epochs'],\n",
        "        'accuracy': experiment_accuracy,\n",
        "        'parameters': sum(p.numel() for p in experiment_model.parameters())\n",
        "    }\n",
        "    experiment_results.append(result)\n",
        "\n",
        "    print(f\"  Final Accuracy: {experiment_accuracy*100:.2f}%\")\n",
        "    print(f\"  Total Parameters: {result['parameters']:,}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5pugVkSLjczS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Exercise 2: Compare on different datasets\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_model_config = {'hidden_units': [16, 8], 'learning_rate': 0.01, 'epochs': 100}\n",
        "\n",
        "exercise2_results = []\n",
        "\n",
        "noise_levels = [0.01, 0.03, 0.05, 0.1]\n",
        "\n",
        "for noise in noise_levels:\n",
        "    print(f\"\\nNoise level: {noise}\")\n",
        "\n",
        "    # Create circles data with different noise\n",
        "    X_circles, y_circles = make_circles(n_samples=1000, noise=noise, factor=0.5, random_state=42)\n",
        "\n",
        "    # Split data\n",
        "    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "        X_circles, y_circles, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_c = torch.FloatTensor(X_train_c).to(device)\n",
        "    y_train_c = torch.FloatTensor(y_train_c).unsqueeze(1).to(device)\n",
        "    X_test_c = torch.FloatTensor(X_test_c).to(device)\n",
        "    y_test_c = torch.FloatTensor(y_test_c).unsqueeze(1).to(device)\n",
        "\n",
        "    # Create and train model\n",
        "    model_c = BinaryClassifier(hidden_units=test_model_config['hidden_units'])\n",
        "    trainer_c = ClassificationTrainer(model_c, device=device)\n",
        "    trainer_c.train(\n",
        "        X_train_c, y_train_c, X_test_c, y_test_c,\n",
        "        epochs=test_model_config['epochs'],\n",
        "        lr=test_model_config['learning_rate']\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy_c = trainer_c.evaluate(X_test_c, y_test_c)\n",
        "\n",
        "    result = {\n",
        "        'dataset': f'make_circles (noise={noise})',\n",
        "        'noise_level': noise,\n",
        "        'accuracy': accuracy_c,\n",
        "        'type': 'circles'\n",
        "    }\n",
        "    exercise2_results.append(result)\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy_c*100:.2f}%\")\n",
        "\n",
        "num_classes_list = [2, 3, 4, 5]\n",
        "\n",
        "for num_classes in num_classes_list:\n",
        "    print(f\"\\nNumber of classes: {num_classes}\")\n",
        "\n",
        "    # Create blobs data with different number of classes\n",
        "    X_blobs, y_blobs = make_blobs(\n",
        "        n_samples=1000,\n",
        "        n_features=2,\n",
        "        centers=num_classes,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Split data\n",
        "    X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n",
        "        X_blobs, y_blobs, test_size=0.2, random_state=42, stratify=y_blobs\n",
        "    )\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_b = torch.FloatTensor(X_train_b).to(device)\n",
        "    y_train_b = torch.LongTensor(y_train_b).to(device)\n",
        "    X_test_b = torch.FloatTensor(X_test_b).to(device)\n",
        "    y_test_b = torch.LongTensor(y_test_b).to(device)\n",
        "\n",
        "    # Create multi-class model\n",
        "    model_b = MultiClassClassifier(\n",
        "        num_classes=num_classes,\n",
        "        hidden_units=test_model_config['hidden_units']\n",
        "    )\n",
        "    model_b = model_b.to(device)\n",
        "\n",
        "    # Train\n",
        "    criterion_b = nn.CrossEntropyLoss()\n",
        "    optimizer_b = optim.Adam(model_b.parameters(), lr=test_model_config['learning_rate'])\n",
        "\n",
        "    for epoch in range(test_model_config['epochs']):\n",
        "        model_b.train()\n",
        "        outputs = model_b(X_train_b)\n",
        "        loss = criterion_b(outputs, y_train_b)\n",
        "        optimizer_b.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_b.step()\n",
        "\n",
        "    # Evaluate\n",
        "    model_b.eval()\n",
        "    with torch.inference_mode():\n",
        "        test_preds_b = torch.argmax(model_b(X_test_b), dim=1)\n",
        "        accuracy_b = (test_preds_b == y_test_b).float().mean().item()\n",
        "\n",
        "    result = {\n",
        "        'dataset': f'make_blobs (classes={num_classes})',\n",
        "        'num_classes': num_classes,\n",
        "        'accuracy': accuracy_b,\n",
        "        'type': 'blobs'\n",
        "    }\n",
        "    exercise2_results.append(result)\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy_b*100:.2f}%\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Im5aXvu-nclk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Exercise 3: Implement early stopping\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class ClassificationTrainerWithEarlyStopping(ClassificationTrainer):\n",
        "\n",
        "    def train(self, X_train, y_train, X_test, y_test,\n",
        "              epochs=100, lr=0.01, patience=10):\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        best_test_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        stopped_epoch = None\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            y_logits = self.model(X_train)\n",
        "            loss = criterion(y_logits, y_train)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Testing\n",
        "            self.model.eval()\n",
        "            with torch.inference_mode():\n",
        "                test_logits = self.model(X_test)\n",
        "                test_loss = criterion(test_logits, y_test)\n",
        "\n",
        "            self.history['train_loss'].append(loss.item())\n",
        "            self.history['test_loss'].append(test_loss.item())\n",
        "\n",
        "            # Early stopping logic\n",
        "            if test_loss.item() < best_test_loss:\n",
        "                best_test_loss = test_loss.item()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}, Patience: {patience_counter}/{patience}')\n",
        "\n",
        "            # Stop if patience exceeded\n",
        "            if patience_counter >= patience:\n",
        "                stopped_epoch = epoch + 1\n",
        "                print(f'Early stopping at epoch {stopped_epoch} (patience: {patience})')\n",
        "                break\n",
        "\n",
        "        self.stopped_epoch = stopped_epoch\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.inference_mode():\n",
        "            test_probs = torch.sigmoid(self.model(X_test))\n",
        "            test_preds = (test_probs > 0.5).long()\n",
        "            accuracy = (test_preds == y_test).float().mean()\n",
        "\n",
        "        return accuracy.item()\n",
        "\n",
        "\n",
        "# Test early stopping with different patience values\n",
        "patience_values = [5, 10, 15, 20]\n",
        "exercise3_results = []\n",
        "\n",
        "print(\"\\nTesting early stopping with different patience values:\")\n",
        "print(\"Using model configuration: hidden=[16, 8], lr=0.01\")\n",
        "\n",
        "for patience in patience_values:\n",
        "    print(f\"\\nPatience: {patience}\")\n",
        "\n",
        "    # Create a fresh model\n",
        "    model_es = BinaryClassifier(hidden_units=[16, 8])\n",
        "\n",
        "    # Create trainer with early stopping\n",
        "    trainer_es = ClassificationTrainerWithEarlyStopping(model_es, device=device)\n",
        "\n",
        "    # Train with early stopping\n",
        "    trainer_es.train(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        epochs=200,\n",
        "        lr=0.01,\n",
        "        patience=patience\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy_es = trainer_es.evaluate(X_test, y_test)\n",
        "\n",
        "    result = {\n",
        "        'patience': patience,\n",
        "        'stopped_epoch': trainer_es.stopped_epoch if hasattr(trainer_es, 'stopped_epoch') else 200,\n",
        "        'accuracy': accuracy_es,\n",
        "        'num_epochs_trained': len(trainer_es.history['train_loss']),\n",
        "        'history': trainer_es.history.copy()\n",
        "    }\n",
        "    exercise3_results.append(result)\n",
        "\n",
        "    print(f\"  Stopped at epoch: {result['stopped_epoch']}\")\n",
        "    print(f\"  Final Accuracy: {accuracy_es*100:.2f}%\")\n",
        "    print(f\"  Total epochs trained: {result['num_epochs_trained']}\")\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1fmVP0Vbo8j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Exercise 4: Create model comparison report\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define different model configurations to compare\n",
        "comparison_configs = [\n",
        "    {'name': 'Linear', 'hidden_units': []},\n",
        "    {'name': 'Small', 'hidden_units': [8]},\n",
        "    {'name': 'Medium', 'hidden_units': [16, 8]},\n",
        "    {'name': 'Large', 'hidden_units': [32, 16, 8]},\n",
        "    {'name': 'Very Large', 'hidden_units': [64, 32, 16]},\n",
        "]\n",
        "\n",
        "exercise4_results = []\n",
        "\n",
        "print(\"\\nTraining and comparing different models...\")\n",
        "print(f\"Dataset: Binary Classification (circles)\")\n",
        "print(f\"Epochs: 100, Learning Rate: 0.01\\n\")\n",
        "\n",
        "for config in comparison_configs:\n",
        "    print(f\"Training {config['name']} model...\")\n",
        "\n",
        "    # Create model\n",
        "    model_comp = BinaryClassifier(hidden_units=config['hidden_units'])\n",
        "    model_comp = model_comp.to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    num_params = sum(p.numel() for p in model_comp.parameters())\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train model\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model_comp.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model_comp.train()\n",
        "        y_logits = model_comp(X_train)\n",
        "        loss = criterion(y_logits, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # End timing\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate\n",
        "    model_comp.eval()\n",
        "    with torch.inference_mode():\n",
        "        test_preds = (torch.sigmoid(model_comp(X_test)) > 0.5).long()\n",
        "        accuracy = (test_preds == y_test).float().mean().item()\n",
        "\n",
        "    result = {\n",
        "        'name': config['name'],\n",
        "        'hidden_units': config['hidden_units'],\n",
        "        'num_parameters': num_params,\n",
        "        'training_time': training_time,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    exercise4_results.append(result)\n",
        "\n",
        "    print(f\" Accuracy: {accuracy*100:.2f}%, Parameters: {num_params:,}, Time: {training_time:.2f}s\\n\")\n",
        "\n",
        "\n",
        "# Generate Comparison Report\n",
        "print(f\"\\n{'Model':<15} {'Architecture':<25} {'Parameters':<15} {'Accuracy':<12} {'Time (s)':<10}\")\n",
        "print(\"-\" * 77)\n",
        "\n",
        "for result in exercise4_results:\n",
        "    arch_str = str(result['hidden_units']) if result['hidden_units'] else \"[Direct Output]\"\n",
        "    print(f\"{result['name']:<15} {arch_str:<25} {result['num_parameters']:<15,} {result['accuracy']*100:>10.2f}% {result['training_time']:>9.2f}s\")\n",
        "\n",
        "# Calculate statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "accuracies = [r['accuracy'] for r in exercise4_results]\n",
        "params = [r['num_parameters'] for r in exercise4_results]\n",
        "times = [r['training_time'] for r in exercise4_results]\n",
        "\n",
        "print(f\"\\nAccuracy:\")\n",
        "print(f\"  Highest: {max(accuracies)*100:.2f}% ({exercise4_results[accuracies.index(max(accuracies))]['name']})\")\n",
        "print(f\"  Lowest:  {min(accuracies)*100:.2f}% ({exercise4_results[accuracies.index(min(accuracies))]['name']})\")\n",
        "print(f\"  Average: {sum(accuracies)/len(accuracies)*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nParameters:\")\n",
        "print(f\"  Highest: {max(params):,} ({exercise4_results[params.index(max(params))]['name']})\")\n",
        "print(f\"  Lowest:  {min(params):,} ({exercise4_results[params.index(min(params))]['name']})\")\n",
        "print(f\"  Average: {sum(params)//len(params):,}\")\n",
        "\n",
        "print(f\"\\nTraining Time:\")\n",
        "print(f\"  Slowest: {max(times):.2f}s ({exercise4_results[times.index(max(times))]['name']})\")\n",
        "print(f\"  Fastest: {min(times):.2f}s ({exercise4_results[times.index(min(times))]['name']})\")\n",
        "print(f\"  Average: {sum(times)/len(times):.2f}s\")\n",
        "\n",
        "# Find best model by accuracy\n",
        "best_accuracy_model = max(exercise4_results, key=lambda x: x['accuracy'])\n",
        "print(f\"\\nBest Model by Accuracy: {best_accuracy_model['name']} ({best_accuracy_model['accuracy']*100:.2f}%)\")\n",
        "\n",
        "# Find most efficient model\n",
        "efficiency = [(r['accuracy'] / r['num_parameters'], r) for r in exercise4_results]\n",
        "most_efficient = max(efficiency, key=lambda x: x[0])[1]\n",
        "print(f\"Most Efficient Model: {most_efficient['name']} (accuracy/parameter: {most_efficient['accuracy']/most_efficient['num_parameters']:.6f})\")\n",
        "\n",
        "# Find fastest model\n",
        "fastest_model = min(exercise4_results, key=lambda x: x['training_time'])\n",
        "print(f\"Fastest Model: {fastest_model['name']} ({fastest_model['training_time']:.2f}s)\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3E0Y-tMsqhsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Exercise 5: Save/load with metadata\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create directory\n",
        "metadata_dir = Path('models_with_metadata')\n",
        "metadata_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"\\nSaving models with metadata...\\n\")\n",
        "\n",
        "models_to_save = [\n",
        "    {\n",
        "        'name': 'small_model',\n",
        "        'config': {'hidden_units': [8]},\n",
        "        'model': BinaryClassifier(hidden_units=[8])\n",
        "    },\n",
        "    {\n",
        "        'name': 'medium_model',\n",
        "        'config': {'hidden_units': [16, 8]},\n",
        "        'model': BinaryClassifier(hidden_units=[16, 8])\n",
        "    },\n",
        "    {\n",
        "        'name': 'large_model',\n",
        "        'config': {'hidden_units': [32, 16, 8]},\n",
        "        'model': BinaryClassifier(hidden_units=[32, 16, 8])\n",
        "    }\n",
        "]\n",
        "\n",
        "saved_models_info = []\n",
        "\n",
        "for model_info in models_to_save:\n",
        "    model_name = model_info['name']\n",
        "    config = model_info['config']\n",
        "    model = model_info['model'].to(device)\n",
        "\n",
        "    print(f\"Processing {model_name}...\")\n",
        "\n",
        "    # Train the model quickly\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        y_logits = model(X_train)\n",
        "        loss = criterion(y_logits, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        test_preds = (torch.sigmoid(model(X_test)) > 0.5).long()\n",
        "        accuracy = (test_preds == y_test).float().mean().item()\n",
        "\n",
        "    # Create metadata dictionary\n",
        "    metadata = {\n",
        "        'model_name': model_name,\n",
        "        'save_datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'hyperparameters': {\n",
        "            'hidden_units': config['hidden_units'],\n",
        "            'learning_rate': 0.01,\n",
        "            'epochs': 100\n",
        "        },\n",
        "        'training_metrics': {\n",
        "            'test_accuracy': accuracy,\n",
        "            'final_train_loss': loss.item()\n",
        "        },\n",
        "        'model_info': {\n",
        "            'num_parameters': sum(p.numel() for p in model.parameters()),\n",
        "            'device': str(device)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save model state dict\n",
        "    model_path = metadata_dir / f'{model_name}_weights.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # Save metadata using pickle\n",
        "    metadata_path = metadata_dir / f'{model_name}_metadata.pkl'\n",
        "    with open(metadata_path, 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    # Also save metadata as JSON for readability\n",
        "    json_path = metadata_dir / f'{model_name}_metadata.json'\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "\n",
        "    saved_models_info.append({\n",
        "        'model_name': model_name,\n",
        "        'weights_path': str(model_path),\n",
        "        'metadata_path': str(metadata_path),\n",
        "        'metadata': metadata\n",
        "    })\n",
        "\n",
        "    print(f\"   Saved: {model_path}\")\n",
        "    print(f\"   Metadata: {metadata_path}\")\n",
        "    print(f\"   JSON: {json_path}\\n\")\n",
        "\n",
        "\n",
        "loaded_models_info = []\n",
        "\n",
        "for saved_info in saved_models_info:\n",
        "    model_name = saved_info['model_name']\n",
        "    weights_path = saved_info['weights_path']\n",
        "    metadata_path = saved_info['metadata_path']\n",
        "\n",
        "    print(f\"Loading {model_name}...\")\n",
        "\n",
        "    # Load metadata from pickle\n",
        "    with open(metadata_path, 'rb') as f:\n",
        "        loaded_metadata = pickle.load(f)\n",
        "\n",
        "    # Load model weights\n",
        "    loaded_model = BinaryClassifier(\n",
        "        hidden_units=loaded_metadata['hyperparameters']['hidden_units']\n",
        "    )\n",
        "    loaded_model.load_state_dict(torch.load(weights_path))\n",
        "    loaded_model = loaded_model.to(device)\n",
        "    loaded_model.eval()\n",
        "\n",
        "    # Verify model works\n",
        "    with torch.inference_mode():\n",
        "        test_preds = (torch.sigmoid(loaded_model(X_test)) > 0.5).long()\n",
        "        accuracy = (test_preds == y_test).float().mean().item()\n",
        "\n",
        "    loaded_models_info.append({\n",
        "        'model_name': model_name,\n",
        "        'metadata': loaded_metadata,\n",
        "        'loaded_accuracy': accuracy\n",
        "    })\n",
        "\n",
        "    print(f\"   Model loaded successfully\")\n",
        "    print(f\"   Verified accuracy: {accuracy*100:.2f}%\\n\")\n",
        "\n",
        "#\n",
        "\n",
        "for saved_info in saved_models_info:\n",
        "    metadata = saved_info['metadata']\n",
        "\n",
        "    print(f\"\\nModel: {metadata['model_name']}\")\n",
        "    print(f\"  Saved: {metadata['save_datetime']}\")\n",
        "    print(f\"  Hyperparameters:\")\n",
        "    print(f\"    - Hidden Units: {metadata['hyperparameters']['hidden_units']}\")\n",
        "    print(f\"    - Learning Rate: {metadata['hyperparameters']['learning_rate']}\")\n",
        "    print(f\"    - Epochs: {metadata['hyperparameters']['epochs']}\")\n",
        "    print(f\"  Training Metrics:\")\n",
        "    print(f\"    - Test Accuracy: {metadata['training_metrics']['test_accuracy']*100:.2f}%\")\n",
        "    print(f\"    - Final Train Loss: {metadata['training_metrics']['final_train_loss']:.4f}\")\n",
        "    print(f\"  Model Info:\")\n",
        "    print(f\"    - Parameters: {metadata['model_info']['num_parameters']:,}\")\n",
        "    print(f\"    - Device: {metadata['model_info']['device']}\")\n",
        "\n",
        "#  Verify loaded models\n",
        "\n",
        "print(f\"\\n{'Model':<20} {'Original Accuracy':<20} {'Loaded Accuracy':<20} {'Match':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, saved_info in enumerate(saved_models_info):\n",
        "    original_acc = saved_info['metadata']['training_metrics']['test_accuracy']\n",
        "    loaded_acc = loaded_models_info[i]['loaded_accuracy']\n",
        "    match = \" Yes\" if abs(original_acc - loaded_acc) < 0.001 else \"âœ— No\"\n",
        "\n",
        "    print(f\"{saved_info['model_name']:<20} {original_acc*100:>18.2f}% {loaded_acc*100:>18.2f}% {match:<10}\")"
      ],
      "metadata": {
        "id": "30R7WcBpq0n-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}