{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5IdmawllCWq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles, make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Part 1: Setup and Data Preparation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create binary classification dataset\n",
        "X, y = make_circles(n_samples=1000, noise=0.03, factor=0.5, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_test = torch.FloatTensor(y_test).unsqueeze(1)\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move to device\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n"
      ],
      "metadata": {
        "id": "jRT2IQaQR3SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 2: Define and Train Model\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define non-linear model\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(2, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model = BinaryClassifier().to(device)\n",
        "\n",
        "# Train model\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    y_logits = model(X_train)\n",
        "    loss = criterion(y_logits, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NG_xLom-WMmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 3: Calculate Metrics with sklearn\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Make predictions\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    test_logits = model(X_test)\n",
        "    test_probs = torch.sigmoid(test_logits)\n",
        "    test_preds = (test_probs > 0.5).long()\n",
        "\n",
        "# Convert to numpy for sklearn\n",
        "y_true = y_test.cpu().numpy().squeeze()\n",
        "y_pred = test_preds.cpu().numpy().squeeze()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"Classification Metrics:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1']))\n"
      ],
      "metadata": {
        "id": "Y_oVLZs7Wh8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 4: Confusion Matrix\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nTP={cm[1,1]}, TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}\")\n",
        "\n",
        "# Explain the confusion matrix\n",
        "print(\"\\nConfusion Matrix Explained:\")\n",
        "print(\"                Predicted\")\n",
        "print(\"           Positive    Negative\")\n",
        "print(\"Actual  Pos    TP         FN\")\n",
        "print(\"        Neg    FP         TN\")\n",
        "print(\"\\nTP = True Positive  (correctly predicted positive)\")\n",
        "print(\"TN = True Negative  (correctly predicted negative)\")\n",
        "print(\"FP = False Positive (incorrectly predicted positive)\")\n",
        "print(\"FN = False Negative (incorrectly predicted negative)\")\n"
      ],
      "metadata": {
        "id": "ZZLIplBjWpXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 5: Visualizing Confusion Matrix\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names=None, title='Confusion Matrix'):\n",
        "    \"\"\"Plot confusion matrix with annotations\"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [f'Class {i}' for i in range(len(cm))]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    # Configure axes\n",
        "    ax.set(\n",
        "        xticks=np.arange(len(class_names)),\n",
        "        yticks=np.arange(len(class_names)),\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        title=title,\n",
        "        ylabel='True Label',\n",
        "        xlabel='Predicted Label'\n",
        "    )\n",
        "\n",
        "    # Rotate x-axis labels\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
        "\n",
        "    # Annotate cells\n",
        "    thresh = cm.max() / 2\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                   ha='center', va='center',\n",
        "                   color='white' if cm[i, j] > thresh else 'black')\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(cm, class_names=['Class 0', 'Class 1'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tkQyhzCfWrI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 6: Understanding Precision vs Recall\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"Precision vs Recall Trade-off:\")\n",
        "print(\"\\nHigh Precision, Low Recall:\")\n",
        "print(\"  - Very confident when predicting positive\")\n",
        "print(\"  - Few false positives\")\n",
        "print(\"  - But misses many actual positives\")\n",
        "print(\"  Example: Spam filter (don't mark legitimate email as spam)\")\n",
        "\n",
        "print(\"\\nLow Precision, High Recall:\")\n",
        "print(\"  - Catches most actual positives\")\n",
        "print(\"  - Many false positives\")\n",
        "print(\"  - Better to be safe than sorry\")\n",
        "print(\"  Example: Disease screening (don't miss sick patients)\")\n",
        "\n",
        "print(f\"\\nThis model:\")\n",
        "print(f\"  Precision: {precision:.4f} - Of all predicted positive, {precision*100:.1f}% are correct\")\n",
        "print(f\"  Recall:    {recall:.4f} - Of all actual positive, {recall*100:.1f}% are caught\")\n"
      ],
      "metadata": {
        "id": "FJu5akQAWwcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 7: Using torchmetrics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    from torchmetrics import Accuracy, Precision, Recall, F1Score, ConfusionMatrix\n",
        "\n",
        "    # Setup metrics\n",
        "    accuracy_metric = Accuracy(task='binary')\n",
        "    precision_metric = Precision(task='binary')\n",
        "    recall_metric = Recall(task='binary')\n",
        "    f1_metric = F1Score(task='binary')\n",
        "    confmat_metric = ConfusionMatrix(task='binary')\n",
        "\n",
        "    # Calculate metrics\n",
        "    acc = accuracy_metric(test_preds, y_test)\n",
        "    prec = precision_metric(test_preds, y_test)\n",
        "    rec = recall_metric(test_preds, y_test)\n",
        "    f1 = f1_metric(test_preds, y_test)\n",
        "    cm_tm = confmat_metric(test_preds, y_test)\n",
        "\n",
        "    print(\"torchmetrics Results:\")\n",
        "    print(f\"  Accuracy:  {acc:.4f}\")\n",
        "    print(f\"  Precision: {prec:.4f}\")\n",
        "    print(f\"  Recall:    {rec:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(f\"  Confusion Matrix:\\n{cm_tm}\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"torchmetrics not installed.\")\n",
        "    print(\"Install with: pip install torchmetrics\")"
      ],
      "metadata": {
        "id": "B-rNuNXwWyc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 8: Multi-Class Metrics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create multi-class dataset\n",
        "X_multi, y_multi = make_blobs(n_samples=1000, n_features=2, centers=4, random_state=42)\n",
        "\n",
        "# Split\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
        "    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
        ")\n",
        "\n",
        "# Convert to tensors\n",
        "X_test_m = torch.FloatTensor(X_test_m).to(device)\n",
        "y_test_m = torch.LongTensor(y_test_m).to(device)\n",
        "\n",
        "# Train simple multi-class model\n",
        "class MultiClassClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiClassClassifier, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(2, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model_m = MultiClassClassifier().to(device)\n",
        "\n",
        "# Train\n",
        "criterion_m = nn.CrossEntropyLoss()\n",
        "optimizer_m = optim.Adam(model_m.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    model_m.train()\n",
        "    outputs = model_m(torch.FloatTensor(X_train_m).to(device))\n",
        "    loss = criterion_m(outputs, torch.LongTensor(y_train_m).to(device))\n",
        "    optimizer_m.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_m.step()\n",
        "\n",
        "# Make predictions\n",
        "model_m.eval()\n",
        "with torch.inference_mode():\n",
        "    test_logits_m = model_m(X_test_m)\n",
        "    test_preds_m = torch.argmax(test_logits_m, dim=1)\n",
        "\n",
        "# Calculate metrics\n",
        "y_true_m = y_test_m.cpu().numpy()\n",
        "y_pred_m = test_preds_m.cpu().numpy()\n",
        "\n",
        "accuracy_m = accuracy_score(y_true_m, y_pred_m)\n",
        "precision_m = precision_score(y_true_m, y_pred_m, average='macro')\n",
        "recall_m = recall_score(y_true_m, y_pred_m, average='macro')\n",
        "f1_m = f1_score(y_true_m, y_pred_m, average='macro')\n",
        "\n",
        "print(\"Multi-class Metrics:\")\n",
        "print(f\"  Accuracy:  {accuracy_m:.4f}\")\n",
        "print(f\"  Precision: {precision_m:.4f} (macro)\")\n",
        "print(f\"  Recall:    {recall_m:.4f} (macro)\")\n",
        "print(f\"  F1-Score:  {f1_m:.4f} (macro)\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm_m = confusion_matrix(y_true_m, y_pred_m)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm_m)\n",
        "\n",
        "# Plot multi-class confusion matrix\n",
        "plot_confusion_matrix(cm_m, class_names=['Class 0', 'Class 1', 'Class 2', 'Class 3'],\n",
        "                     title='Multi-class Confusion Matrix')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4u-hoOnGW4V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Part 9: Comparing Models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"When comparing models, use multiple metrics:\")\n",
        "print(\"  1. Accuracy - Overall correctness\")\n",
        "print(\"  2. Precision - How many predicted positives are correct\")\n",
        "print(\"  3. Recall - How many actual positives were caught\")\n",
        "print(\"  4. F1-Score - Balance between precision and recall\")\n",
        "\n",
        "print(\"\\nWhich metric matters most depends on your problem:\")\n",
        "print(\"  - Spam detection: High precision (don't delete legitimate emails)\")\n",
        "print(\"  - Disease screening: High recall (don't miss sick patients)\")\n",
        "print(\"  - Balanced dataset: Accuracy is fine\")\n",
        "print(\"  - Imbalanced dataset: Use F1-score or precision/recall\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MSiBVZJtW41Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Create IMBALANCED dataset (90% class 0, 10% class 1)\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=2,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.9, 0.1],  # 90% class 0, 10% class 1\n",
        "    flip_y=0.01,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Check class distribution\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(f\"\\nClass distribution:\")\n",
        "for cls, count in zip(unique, counts):\n",
        "    print(f\"  Class {cls}: {count} samples ({count/len(y)*100:.1f}%)\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_test = torch.FloatTensor(y_test).unsqueeze(1)\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "model = BinaryClassifier().to(device)\n",
        "\n",
        "# Train model\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"\\nTraining model...\")\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    y_logits = model(X_train)\n",
        "    loss = criterion(y_logits, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Make predictions\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    test_logits = model(X_test)\n",
        "    test_probs = torch.sigmoid(test_logits)\n",
        "    test_preds = (test_probs > 0.5).long()\n",
        "\n",
        "# Convert to numpy\n",
        "y_true = y_test.cpu().numpy().squeeze()\n",
        "y_pred = test_preds.cpu().numpy().squeeze()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Results: Accuracy vs F1-Score\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Show confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"TP={cm[1,1]}, TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}\")\n",
        "\n",
        "\n",
        "# Calculate naive baseline\n",
        "baseline_accuracy = np.sum(y_true == 0) / len(y_true)\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"Comparison with Naive Baseline\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Naive baseline : {baseline_accuracy:.4f}\")\n",
        "print(f\"Model accuracy:                           {accuracy:.4f}\")\n",
        "print(f\"Model F1-Score:                           {f1:.4f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9ntF2d_gW6-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining model...\")\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    y_logits = model(X_train)\n",
        "    loss = criterion(y_logits, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Make predictions (get probabilities)\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    test_logits = model(X_test)\n",
        "    test_probs = torch.sigmoid(test_logits)\n",
        "\n",
        "# Convert to numpy\n",
        "y_true = y_test.cpu().numpy().squeeze()\n",
        "test_probs_np = test_probs.cpu().numpy().squeeze()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Testing Different Thresholds\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test different thresholds\n",
        "thresholds = [0.3, 0.5, 0.7]\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Apply threshold\n",
        "    y_pred = (test_probs_np > threshold).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\nThreshold: {threshold}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EG0nZ6WVZKCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare different averaging methods\n",
        "averaging_methods = ['macro', 'weighted', 'micro']\n",
        "\n",
        "print(\"\\nComparing Averaging Methods:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for avg_method in averaging_methods:\n",
        "    precision = precision_score(y_true_m, y_pred_m, average=avg_method)\n",
        "    recall = recall_score(y_true_m, y_pred_m, average=avg_method)\n",
        "    f1 = f1_score(y_true_m, y_pred_m, average=avg_method)\n",
        "\n",
        "    print(f\"\\nAverage method: '{avg_method}'\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "id": "si937DFpayih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Exercise 4: Create Custom Metrics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Extract values from confusion matrix\n",
        "TN = cm[0, 0]\n",
        "FP = cm[0, 1]\n",
        "FN = cm[1, 0]\n",
        "TP = cm[1, 1]\n",
        "\n",
        "print(f\"\\nConfusion Matrix Values:\")\n",
        "print(f\"  TN (True Negative):  {TN}\")\n",
        "print(f\"  FP (False Positive): {FP}\")\n",
        "print(f\"  FN (False Negative): {FN}\")\n",
        "print(f\"  TP (True Positive):  {TP}\")\n",
        "\n",
        "# Custom Metric 1: Specificity\n",
        "specificity = TN / (TN + FP)\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"Custom Metric 1: Specificity\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Specificity = TN / (TN + FP)\")\n",
        "print(f\"Specificity = {TN} / ({TN} + {FP})\")\n",
        "print(f\"Specificity = {specificity:.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"  Of all actual negatives, {specificity*100:.1f}% were correctly identified\")\n",
        "print(\"  High specificity = few false alarms\")\n",
        "\n",
        "# Custom Metric 2: False Positive Rate (FPR)\n",
        "fpr = FP / (FP + TN)\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"Custom Metric 2: False Positive Rate\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"FPR = FP / (FP + TN)\")\n",
        "print(f\"FPR = {FP} / ({FP} + {TN})\")\n",
        "print(f\"FPR = {fpr:.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(f\"  Of all actual negatives, {fpr*100:.1f}% were incorrectly labeled positive\")\n",
        "print(\"  Low FPR = good (fewer false alarms)\")\n",
        "\n",
        "# Relationship\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"Relationship:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Specificity + FPR = {specificity:.4f} + {fpr:.4f} = {specificity + fpr:.4f}\")\n",
        "print(\"Specificity = 1 - FPR\")\n",
        "print(\"(They are complementary)\")\n",
        "\n",
        "# Compare with Recall (Sensitivity)\n",
        "recall = TP / (TP + FN)\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"Comparison with Recall:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Recall (Sensitivity):     {recall:.4f} - focuses on positives\")\n",
        "print(f\"Specificity:              {specificity:.4f} - focuses on negatives\")\n",
        "print(\"\\nBoth are important for a complete picture!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Jk47WVW5cwMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_m = confusion_matrix(y_true_m, y_pred_m)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_m)\n",
        "\n",
        "# Calculate per-class metrics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Per-class Metrics:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "n_classes = len(np.unique(y_true_m))\n",
        "\n",
        "for class_id in range(n_classes):\n",
        "    # Calculate metrics for each class using one-vs-rest approach\n",
        "    precision = precision_score(y_true_m, y_pred_m, labels=[class_id], average=None)[0]\n",
        "    recall = recall_score(y_true_m, y_pred_m, labels=[class_id], average=None)[0]\n",
        "    f1 = f1_score(y_true_m, y_pred_m, labels=[class_id], average=None)[0]\n",
        "\n",
        "    # Count samples\n",
        "    n_samples = np.sum(y_true_m == class_id)\n",
        "\n",
        "    print(f\"\\nClass {class_id} ({n_samples} samples):\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Find hardest class to classify\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Analysis: Which Class is Hardest to Classify?\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate F1 for all classes\n",
        "f1_per_class = f1_score(y_true_m, y_pred_m, average=None)\n",
        "\n",
        "hardest_class = np.argmin(f1_per_class)\n",
        "easiest_class = np.argmax(f1_per_class)\n",
        "\n",
        "print(f\"\\nHardest class: Class {hardest_class}\")\n",
        "print(f\"  F1-Score: {f1_per_class[hardest_class]:.4f}\")\n",
        "\n",
        "print(f\"\\nEasiest class: Class {easiest_class}\")\n",
        "print(f\"  F1-Score: {f1_per_class[easiest_class]:.4f}\")\n",
        "\n",
        "# Per-class accuracy from confusion matrix\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Per-class Accuracy (from confusion matrix):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for class_id in range(n_classes):\n",
        "    # Diagonal element = correctly classified\n",
        "    correct = cm_m[class_id, class_id]\n",
        "    total = np.sum(cm_m[class_id, :])\n",
        "    class_accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    print(f\"Class {class_id}: {class_accuracy:.4f} ({correct}/{total} correct)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Conclusion:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"The hardest class is Class {hardest_class} with F1={f1_per_class[hardest_class]:.4f}\")\n",
        "print(\"This could be due to:\")\n",
        "print(\"  - Overlapping features with other classes\")\n",
        "print(\"  - Fewer training samples\")\n",
        "print(\"  - Inherent difficulty in the data\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qAd-_SKLdHf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output(wait=True)"
      ],
      "metadata": {
        "id": "EtcO3duZhDOQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}